{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57c5e2c-04f8-40b7-9b47-e5e05505cb2c",
   "metadata": {
    "id": "e57c5e2c-04f8-40b7-9b47-e5e05505cb2c"
   },
   "source": [
    "# Training a Diffusion Model with Weights and Biases (W&B)\n",
    "\n",
    "<!--- @wandbcode{dlai_02} -->\n",
    "\n",
    "In this notebooks we will instrument the training of a diffusion model with W&B. We will use the Lab3 notebook from the [\"How diffusion models work\"](https://www.deeplearning.ai/short-courses/how-diffusion-models-work/) course.\n",
    "We will add:\n",
    "- Logging of the training loss and metrics\n",
    "- Sampling from the model during training and uploading the samples to W&B\n",
    "- Saving the model checkpoints to W&B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SEqVFedoiXyo",
   "metadata": {
    "id": "SEqVFedoiXyo"
   },
   "source": [
    "Training a diffusion model \\\n",
    "Tracking progress with W&B\n",
    "- NN learns to predict noise -- really learns the distribution of what is not noise\n",
    "- Sample random timestep (noise level) per image to train more stably.\n",
    "- a diffusion model learns how to iteratively remove small amounts of noise from an image.\n",
    "- we use the same code as on the \"How diffusion models work\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KSaDHcBhi34A",
   "metadata": {
    "id": "KSaDHcBhi34A"
   },
   "source": [
    "# Training a diffusion model\n",
    "tracking progress with W&B\n",
    "- telemetry is very important when it comes to training generative models;\n",
    "- for the diffusion training we can:\n",
    "  - keep track of the loss and relevant metrics.\n",
    "  - sample images from teh model during training\n",
    "  - safely store and version model checkpoints.\n",
    "\n",
    "\n",
    "Due to diffusion model's complexity and time-consuming traits, it is crutial to check the progress of the model during training.\n",
    "\n",
    "\n",
    "Bascially, doing all the housekeeping work for you while you do the heavylifting of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a34666-2281-49e3-8574-93d57c72771b",
   "metadata": {
    "height": 183,
    "id": "d4a34666-2281-49e3-8574-93d57c72771b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from utilities_ import *\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904d68fe-7435-48a3-b8af-c4be8675311c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "height": 30,
    "id": "904d68fe-7435-48a3-b8af-c4be8675311c",
    "outputId": "c7023a07-7491-40d1-bf76-64707af3533b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2b5b2-82e4-4535-aa98-34ae64a808e8",
   "metadata": {
    "id": "02e2b5b2-82e4-4535-aa98-34ae64a808e8"
   },
   "source": [
    "## Setting Things Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4918eda7-6d6b-4f9f-8650-c347ed4a5d1c",
   "metadata": {
    "height": 438,
    "id": "4918eda7-6d6b-4f9f-8650-c347ed4a5d1c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we are storing the parameters to be logged to wandb\n",
    "DATA_DIR = Path('data/')\n",
    "SAVE_DIR = Path('data/weights/')\n",
    "SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    # hyperparameters\n",
    "    num_samples = 30,\n",
    "\n",
    "    # diffusion hyperparameters\n",
    "    timesteps = 500,\n",
    "    beta1 = 1e-4,\n",
    "    beta2 = 0.02,\n",
    "\n",
    "    # network hyperparameters\n",
    "    n_feat = 64, # 64 hidden dimension feature\n",
    "    n_cfeat = 5, # context vector is of size 5\n",
    "    height = 16, # 16x16 image\n",
    "\n",
    "    # training hyperparameters\n",
    "    batch_size = 100,\n",
    "    n_epoch = 32,\n",
    "    lrate = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed92a7b-b6a3-4c0c-a35d-154ec26ed923",
   "metadata": {
    "id": "1ed92a7b-b6a3-4c0c-a35d-154ec26ed923"
   },
   "source": [
    "### Setup DDPM noise scheduler and sampler (same as in the Diffusion course).\n",
    "- perturb_input: Adds noise to the input image at the corresponding timestep on the schedule\n",
    "- sample_ddpm_context: Generate images using the DDPM sampler, we will use this function during training to sample from the model regularly and see how our training is progressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba81b76-6521-4c7c-80bd-bacde0361a34",
   "metadata": {
    "height": 113,
    "id": "5ba81b76-6521-4c7c-80bd-bacde0361a34"
   },
   "outputs": [],
   "source": [
    "# setup ddpm sampler functions\n",
    "perturb_input, sample_ddpm_context = setup_ddpm(config.beta1,\n",
    "                                                config.beta2,\n",
    "                                                config.timesteps,\n",
    "                                                DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83bd768-f709-410a-8062-703bde7997d8",
   "metadata": {
    "height": 98,
    "id": "c83bd768-f709-410a-8062-703bde7997d8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3,\n",
    "                       n_feat=config.n_feat,\n",
    "                       n_cfeat=config.n_cfeat,\n",
    "                       height=config.height).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf98a114-f7aa-4cbd-b08c-d56ad628da21",
   "metadata": {
    "height": 130,
    "id": "cf98a114-f7aa-4cbd-b08c-d56ad628da21"
   },
   "outputs": [],
   "source": [
    "# load dataset and construct optimizer\n",
    "dataset = CustomDataset.from_np(path=DATA_DIR)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=config.batch_size,\n",
    "                        shuffle=True)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=config.lrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdccd6e0-850a-41ed-89e7-db629f838770",
   "metadata": {
    "id": "bdccd6e0-850a-41ed-89e7-db629f838770"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338bec6-319c-4603-8ae6-0e1fcbdd3a4e",
   "metadata": {
    "id": "2338bec6-319c-4603-8ae6-0e1fcbdd3a4e"
   },
   "source": [
    "We choose a fixed context vector with 6 samples of each class to guide our diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56bfcd32-1a9c-4d0e-8237-77da217f41ae",
   "metadata": {
    "height": 232,
    "id": "56bfcd32-1a9c-4d0e-8237-77da217f41ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Noise vector\n",
    "# x_T ~ N(0, 1), sample initial noise\n",
    "noises = torch.randn(config.num_samples, 3,\n",
    "                     config.height, config.height).to(DEVICE)\n",
    "\n",
    "# A fixed context vector to sample from\n",
    "ctx_vector = F.one_hot(torch.tensor([0,0,0,0,0,0,   # hero\n",
    "                                     1,1,1,1,1,1,   # non-hero\n",
    "                                     2,2,2,2,2,2,   # food\n",
    "                                     3,3,3,3,3,3,   # spell\n",
    "                                     4,4,4,4,4,4]), # side-facing\n",
    "                       5).to(DEVICE).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854b7c7-fa0d-4413-8642-f824449d6763",
   "metadata": {
    "id": "e854b7c7-fa0d-4413-8642-f824449d6763"
   },
   "source": [
    "The following training cell takes very long to run on CPU, we have already trained the model for you on a GPU equipped machine.\n",
    "\n",
    "### You can visit the result of this >> [training here](https://wandb.ai/dlai-course/dlai_sprite_diffusion/runs/pzs3gsyo) <<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c87ca8f-2c09-487f-a8bc-7030c2b76492",
   "metadata": {
    "height": 946,
    "id": "2c87ca8f-2c09-487f-a8bc-7030c2b76492",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a wandb run\n",
    "run = wandb.init(project=\"dlai_sprite_diffusion\",\n",
    "                 job_type=\"train\",\n",
    "                 config=config)\n",
    "\n",
    "# we pass the config back from W&B\n",
    "config = wandb.config\n",
    "\n",
    "for ep in tqdm(range(config.n_epoch), leave=True, total=config.n_epoch):\n",
    "    # set into train mode\n",
    "    nn_model.train()\n",
    "    optim.param_groups[0]['lr'] = config.lrate*(1-ep/config.n_epoch)\n",
    "\n",
    "    pbar = tqdm(dataloader, leave=False)\n",
    "    for x, c in pbar:   # x: images  c: context\n",
    "        optim.zero_grad()\n",
    "        x = x.to(DEVICE)\n",
    "        c = c.to(DEVICE)\n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.8).to(DEVICE)\n",
    "        c = c * context_mask.unsqueeze(-1)\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, config.timesteps + 1, (x.shape[0],)).to(DEVICE)\n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "        pred_noise = nn_model(x_pert, t / config.timesteps, c=c)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        wandb.log({\"loss\": loss.item(),\n",
    "                   \"lr\": optim.param_groups[0]['lr'],\n",
    "                   \"epoch\": ep})\n",
    "\n",
    "\n",
    "    # save model periodically\n",
    "    if ep%4==0 or ep == int(config.n_epoch-1):\n",
    "        nn_model.eval()\n",
    "        ckpt_file = SAVE_DIR/f\"context_model.pth\"\n",
    "        torch.save(nn_model.state_dict(), ckpt_file)\n",
    "\n",
    "        artifact_name = f\"{wandb.run.id}_context_model\"\n",
    "        at = wandb.Artifact(artifact_name, type=\"model\")\n",
    "        at.add_file(ckpt_file)\n",
    "        wandb.log_artifact(at, aliases=[f\"epoch_{ep}\"])\n",
    "\n",
    "        samples, _ = sample_ddpm_context(nn_model,\n",
    "                                         noises,\n",
    "                                         ctx_vector[:config.num_samples])\n",
    "        wandb.log({\n",
    "            \"train_samples\": [\n",
    "                wandb.Image(img) for img in samples.split(1)\n",
    "            ]})\n",
    "\n",
    "# finish W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aUr7-Q2Ah6R7",
   "metadata": {
    "id": "aUr7-Q2Ah6R7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "jJSBoom_kYGz",
   "metadata": {
    "id": "jJSBoom_kYGz"
   },
   "source": [
    "On the dashboard, open artifacts to see the metadat and be able to see the lineage, and will be able to share with teammates etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cIf3zFlkfrc",
   "metadata": {
    "id": "6cIf3zFlkfrc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
